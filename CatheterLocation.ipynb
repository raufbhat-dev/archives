{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport ast\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance\nimport numpy as np\nimport copy\nimport random\nimport time\nimport math\nimport sys\n\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import multiprocessing\n\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom torchvision import models, transforms\nfrom torchvision.transforms import functional as F\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport torchvision.models.detection.mask_rcnn","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict, deque\nimport datetime\nimport pickle\nimport time\n\nimport torch\nimport torch.distributed as dist\n\nimport errno\nimport os\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)\n\n\ndef all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(\"cuda\")\n\n    # obtain Tensor size of each rank\n    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n    if local_size != max_size:\n        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.all_reduce(values)\n        if average:\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        if torch.cuda.is_available():\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}',\n                'max mem: {memory:.0f}'\n            ])\n        else:\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}'\n            ])\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndef warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n\n\ndef mkdir(path):\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n    print('| distributed init (rank {}): {}'.format(\n        args.rank, args.dist_url), flush=True)\n    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                         world_size=args.world_size, rank=args.rank)\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiprocessing.set_sharing_strategy('file_system')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = Path('/kaggle/input/ranzcr-clip-catheter-line-classification')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[x for x in DATA_DIR.glob('*.csv')]","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"[PosixPath('/kaggle/input/ranzcr-clip-catheter-line-classification/sample_submission.csv'),\n PosixPath('/kaggle/input/ranzcr-clip-catheter-line-classification/train_annotations.csv'),\n PosixPath('/kaggle/input/ranzcr-clip-catheter-line-classification/train.csv')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(DATA_DIR/ 'sample_submission.csv')\ntrain_annotations = pd.read_csv(DATA_DIR/ 'train_annotations.csv')\ntrain = pd.read_csv(DATA_DIR/ 'train.csv')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bounding_box(cordinates):\n    xmin = ymin = 99999\n    xmax = ymax = -1\n    for x, y  in cordinates:\n        if x < xmin:\n            xmin = x\n        if x > xmax:\n            xmax = x\n        if y < ymin:\n            ymin = y\n        if y > ymax:\n            ymax = y \n       \n    return [xmin,ymin, xmax,ymax]        ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(DATA_DIR/ 'train_annotations.csv')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_strlist2tuple(mask_string):\n    mask_list = ast.literal_eval(mask_string)\n    mask = []\n    for item in mask_list:\n        mask.append( (1 if item[0]<0 else item[0], 1 if item[1]<0 else item[1]))\n    return mask","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(DATA_DIR/ 'train_annotations.csv')\ndata['mask'] = data['data'].apply(lambda x: convert_strlist2tuple(x))\ndata['bounding_box'] = data['mask'].apply(lambda x: get_bounding_box(x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_landmarks(StudyInstanceUID):\n    _sample = data[data.StudyInstanceUID == StudyInstanceUID]\n    if _sample.shape[0] == 0:\n        print(\"Landmarks absent\")\n    image_file = f'{StudyInstanceUID}.jpg'\n    image = Image.open(DATA_DIR / 'train'/ image_file).convert(\"RGB\")\n    mask =  Image.new('L', image.size, color =0)\n    font = ImageFont.truetype('/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf',size=80)\n    draw_img = ImageDraw.Draw(image)\n    draw_mask = ImageDraw.Draw(mask)\n    label = []\n    for index, row in _sample.iterrows():\n        r,g,b = np.random.randint(0,255,3)\n        landmarks = row['mask']\n        bounding_box = row.bounding_box\n        draw_img.line(landmarks, fill=(r,g,b) ,width =5)\n        draw_mask.line(landmarks, fill = int(r) ,width =5)\n        draw_img.text(landmarks[int((len(landmarks)/2))], row.label, font=font, fill= (r,g,b))\n        draw_img.rectangle(bounding_box, outline=(r,g,b) ,width =15)\n        \n    \n    basewidth =512\n    wpercent = (basewidth/float(image.size[0]))\n    hsize = int((float(image.size[1])*float(wpercent)))\n    image = image.resize((basewidth,hsize), Image.ANTIALIAS)\n    mask = mask.resize((basewidth,hsize), Image.ANTIALIAS)\n    return image, mask","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, mask = draw_landmarks(train.StudyInstanceUID.values[10])","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_data = data.groupby(['StudyInstanceUID']).agg({'label':list, 'bounding_box':list, 'mask':list}).reset_index()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_label_map = {'ETT - Abnormal': 'ETT', 'ETT - Borderline': 'ETT', 'ETT - Normal': 'ETT' ,\n                                 'NGT - Abnormal': 'NGT', 'NGT - Borderline': 'NGT', 'NGT - Normal': 'NGT' ,'NGT - Incompletely Imaged':'NGT',\n                                 'CVC - Abnormal': 'CVC', 'CVC - Borderline': 'CVC', 'CVC - Normal': 'CVC',\n                                 'Swan Ganz Catheter Present':'SGC'}\n\nreverse_column_label_map = {'ETT' : ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal'],\n                                    'NGT' : ['NGT - Abnormal', 'NGT - Borderline', 'NGT - Normal' ,'NGT - Incompletely Imaged'],\n                                    'CVC' : ['CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal'],\n                                    'SGC' : ['Swan Ganz Catheter Present']}","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_count = data.label.value_counts()\nlabel_count = pd.DataFrame({'Type': label_count.index ,'Count':label_count.values})\nlabel_count['Label'] = label_count.Type.apply(lambda x: column_label_map[x])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_freq = data.label.value_counts(normalize=True) * 100\nlabel_freq = pd.DataFrame({'Type': label_freq.index ,'Percentage':label_freq.values})\nlabel_freq['Label'] = label_freq.Type.apply(lambda x: column_label_map[x])\nlabel_data = label_count.merge(label_freq, how='inner', on='Type',suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)').sort_values('Label')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomHorizontalFlip(object):\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, mask, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            mask = mask.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n                \n        return image, mask, target","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Rescale(object):\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, image, mask, target):\n        h, w = image.size[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = transforms.Resize((new_h, new_w))(image)\n        if mask:\n            mask = transforms.Resize((new_h, new_w))(mask)\n        for index in range(len(target['boxes'])):\n            xmin = int(target['boxes'][index][0]*(new_w / w))\n            ymin = int(target['boxes'][index][1]*(new_h / h))\n            xmax = int(target['boxes'][index][2]*(new_w / w))\n            ymax = int(target['boxes'][index][3]*(new_h / h))\n            target['boxes'][index][0] = xmin if xmin !=0 else 1\n            target['boxes'][index][1] = ymin if ymin !=0 else 1\n            target['boxes'][index][2] = xmax if xmax !=0 else 1\n            target['boxes'][index][3] = ymax if ymax !=0 else 1\n        return img, mask, target","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToTensor(object):\n    def __call__(self, image, mask, target):\n        image = transforms.ToTensor()(image)\n        mask = transforms.ToTensor()(mask)\n        data_types = {'boxes': torch.float32, 'labels':torch.int64}\n        for k in target.keys():\n                target[k] = torch.as_tensor(target[k], dtype=data_types[k])\n        return image, mask, target","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, mask,  target):\n        for t in self.transforms:\n            image, mask, target = t(image, mask, target)\n        return image, mask, target","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transform(train):\n    transforms = [Rescale((512,512)) ,ToTensor() ]\n# RandomHorizontalFlip(0.3)\n#                                        Normalize([0.485, 0.456, 0.406],\n#                                                             [0.229, 0.224, 0.225])\n    return Compose(transforms)    ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RanzcrDataSet(Dataset):\n    def __init__(self, DATA_DIR, partition, transforms = None):\n        super().__init__()\n        self.DATA_DIR = DATA_DIR \n        self.partition = partition\n        self.column_label_map = {'ETT - Abnormal': 1, 'ETT - Borderline': 1, 'ETT - Normal': 1 ,\n                                 'NGT - Abnormal': 2, 'NGT - Borderline': 2, 'NGT - Normal': 2 ,'NGT - Incompletely Imaged':2,\n                                 'CVC - Abnormal': 3, 'CVC - Borderline': 3, 'CVC - Normal': 3,\n                                 'Swan Ganz Catheter Present':4}\n\n        _data = pd.read_csv(self.DATA_DIR/ 'train_annotations.csv')\n        convert_strlist2tuple = lambda x: [(*li, ) for li in ast.literal_eval(x)]\n        _data['mask'] = _data['data'].apply(lambda x: convert_strlist2tuple(x))\n        _data['bounding_box'] = _data['mask'].apply(lambda x: RanzcrDataSet.get_bounding_box(x))\n        self.data = _data.groupby(['StudyInstanceUID']).agg({'label':list, 'bounding_box':list, 'mask':list}).reset_index()\n        self.transforms = transforms\n\n    @staticmethod\n    def get_bounding_box(cordinates):\n        xmin = ymin = 99999\n        xmax = ymax = -1\n        for x, y  in cordinates:\n            if x < xmin:\n                xmin = x\n            if x > xmax:\n                xmax = x\n            if y < ymin:\n                ymin = y\n            if y > ymax:\n                ymax = y \n        return [xmin,ymin, xmax,ymax]        \n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_file = f'{self.data.StudyInstanceUID[idx]}.jpg' \n        img = Image.open(self.DATA_DIR / self.partition / img_file).convert('RGB')\n        mask =  Image.new('L', img.size, color =0)\n        draw_mask = ImageDraw.Draw(mask)\n        for landmarks in self.data['mask'][idx]:\n            draw_mask.line(landmarks, fill = 255  ,width =8)\n\n        boxes = self.data['bounding_box'][idx]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n        labels = []\n        for lb in self.data['label'][idx]:\n            labels.append(self.column_label_map[lb])\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n\n        if self.transforms:\n            img, mask, target = self.transforms(img, mask, target)\n        \n        mask = np.array(mask)\n        obj_ids = np.unique(mask)\n        obj_ids = obj_ids[1:]\n        masks = mask == obj_ids[:, None, None]\n        num_objs = len(obj_ids)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        area = torch.abs((boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]))\n        \n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target[\"masks\"] = masks\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        target[\"image_id\"] = image_id\n\n        return img, target\n","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(model, optimizer, lr_scheduler, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n#     lr_scheduler = None\n#     if epoch == 0:\n#         warmup_factor = 1. / 1000\n#         warmup_iters = min(1000, len(data_loader) - 1)\n\n#         lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        try:\n            loss_dict = model(images, targets)\n        except Exception as e:\n            continue\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n       \n    return metric_logger","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = RanzcrDataSet(DATA_DIR,'train', get_transform(True))\ndataset_test = RanzcrDataSet(DATA_DIR,'train', get_transform(False))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=1, shuffle=False, num_workers=12,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=12,\n    collate_fn=collate_fn)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model_instance_segmentation(5)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.002,\n                            momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=1000,\n                                               gamma=0.1)","execution_count":30,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/170M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97fca0d642b74e8abff0169a286d12a1"}},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.to(device)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"MaskRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (layer_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign()\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n    )\n    (mask_roi_pool): MultiScaleRoIAlign()\n    (mask_head): MaskRCNNHeads(\n      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu1): ReLU(inplace=True)\n      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu2): ReLU(inplace=True)\n      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu3): ReLU(inplace=True)\n      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu4): ReLU(inplace=True)\n    )\n    (mask_predictor): MaskRCNNPredictor(\n      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (relu): ReLU(inplace=True)\n      (mask_fcn_logits): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 1000 iterations\n    train_one_epoch(model, optimizer,None, data_loader, device, epoch, print_freq=1000)\n    # update the learning rate\n#     lr_scheduler.step()","execution_count":null,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n","name":"stderr"},{"output_type":"stream","text":"Epoch: [0]  [   0/8995]  eta: 22:41:24  lr: 0.002000  loss: 5.8305 (5.8305)  loss_classifier: 1.4100 (1.4100)  loss_box_reg: 0.1768 (0.1768)  loss_mask: 3.8874 (3.8874)  loss_objectness: 0.3230 (0.3230)  loss_rpn_box_reg: 0.0333 (0.0333)  time: 9.0811  data: 5.7715  max mem: 1567\nEpoch: [0]  [1000/8995]  eta: 0:45:27  lr: 0.002000  loss: 0.2794 (0.4487)  loss_classifier: 0.0799 (0.1030)  loss_box_reg: 0.0837 (0.0912)  loss_mask: 0.0075 (0.0187)  loss_objectness: 0.0686 (0.1374)  loss_rpn_box_reg: 0.0113 (0.0985)  time: 0.3217  data: 0.0332  max mem: 1993\nEpoch: [0]  [2000/8995]  eta: 0:39:16  lr: 0.002000  loss: 0.3115 (0.4001)  loss_classifier: 0.1083 (0.0979)  loss_box_reg: 0.1277 (0.0932)  loss_mask: 0.0077 (0.0133)  loss_objectness: 0.0435 (0.1093)  loss_rpn_box_reg: 0.0208 (0.0864)  time: 0.3111  data: 0.0305  max mem: 1996\nEpoch: [0]  [3000/8995]  eta: 0:33:29  lr: 0.002000  loss: 0.2245 (0.3854)  loss_classifier: 0.0859 (0.0984)  loss_box_reg: 0.0806 (0.0962)  loss_mask: 0.0088 (0.0115)  loss_objectness: 0.0660 (0.0978)  loss_rpn_box_reg: 0.0098 (0.0815)  time: 0.3367  data: 0.0313  max mem: 2018\nEpoch: [0]  [4000/8995]  eta: 0:27:55  lr: 0.002000  loss: 0.1848 (0.3677)  loss_classifier: 0.0658 (0.0965)  loss_box_reg: 0.0832 (0.0985)  loss_mask: 0.0069 (0.0105)  loss_objectness: 0.0237 (0.0853)  loss_rpn_box_reg: 0.0066 (0.0769)  time: 0.3413  data: 0.0375  max mem: 2027\nEpoch: [0]  [5000/8995]  eta: 0:22:19  lr: 0.002000  loss: 0.3319 (0.3609)  loss_classifier: 0.1041 (0.0954)  loss_box_reg: 0.1065 (0.1009)  loss_mask: 0.0066 (0.0100)  loss_objectness: 0.0395 (0.0791)  loss_rpn_box_reg: 0.0186 (0.0756)  time: 0.3242  data: 0.0346  max mem: 2027\nEpoch: [0]  [6000/8995]  eta: 0:16:43  lr: 0.002000  loss: 0.1596 (0.3557)  loss_classifier: 0.0488 (0.0950)  loss_box_reg: 0.0874 (0.1032)  loss_mask: 0.0082 (0.0095)  loss_objectness: 0.0168 (0.0739)  loss_rpn_box_reg: 0.0104 (0.0740)  time: 0.3462  data: 0.0351  max mem: 2027\nEpoch: [0]  [7000/8995]  eta: 0:11:08  lr: 0.002000  loss: 0.1703 (0.3472)  loss_classifier: 0.0449 (0.0939)  loss_box_reg: 0.0879 (0.1040)  loss_mask: 0.0067 (0.0092)  loss_objectness: 0.0183 (0.0689)  loss_rpn_box_reg: 0.0032 (0.0712)  time: 0.3468  data: 0.0388  max mem: 2034\nEpoch: [0]  [8000/8995]  eta: 0:05:33  lr: 0.002000  loss: 0.2236 (0.3399)  loss_classifier: 0.0670 (0.0927)  loss_box_reg: 0.1090 (0.1048)  loss_mask: 0.0062 (0.0089)  loss_objectness: 0.0195 (0.0646)  loss_rpn_box_reg: 0.0087 (0.0689)  time: 0.3320  data: 0.0411  max mem: 2034\nEpoch: [0]  [8994/8995]  eta: 0:00:00  lr: 0.002000  loss: 0.2192 (0.3364)  loss_classifier: 0.0715 (0.0916)  loss_box_reg: 0.0812 (0.1054)  loss_mask: 0.0089 (0.0088)  loss_objectness: 0.0362 (0.0623)  loss_rpn_box_reg: 0.0119 (0.0684)  time: 0.1902  data: 0.0111  max mem: 2034\nEpoch: [0] Total time: 0:50:06 (0.3343 s / it)\nEpoch: [1]  [   0/8995]  eta: 11:27:43  lr: 0.002000  loss: 0.2354 (0.2354)  loss_classifier: 0.0680 (0.0680)  loss_box_reg: 0.1168 (0.1168)  loss_mask: 0.0116 (0.0116)  loss_objectness: 0.0193 (0.0193)  loss_rpn_box_reg: 0.0197 (0.0197)  time: 4.5874  data: 3.8619  max mem: 2034\nEpoch: [1]  [1000/8995]  eta: 0:45:44  lr: 0.002000  loss: 0.1967 (0.3072)  loss_classifier: 0.0787 (0.0807)  loss_box_reg: 0.0788 (0.1023)  loss_mask: 0.0069 (0.0072)  loss_objectness: 0.0213 (0.0485)  loss_rpn_box_reg: 0.0129 (0.0684)  time: 0.3672  data: 0.0391  max mem: 2034\nEpoch: [1]  [2000/8995]  eta: 0:39:36  lr: 0.002000  loss: 0.2389 (0.3023)  loss_classifier: 0.0808 (0.0817)  loss_box_reg: 0.1043 (0.1074)  loss_mask: 0.0066 (0.0072)  loss_objectness: 0.0230 (0.0425)  loss_rpn_box_reg: 0.0203 (0.0635)  time: 0.3108  data: 0.0359  max mem: 2034\nEpoch: [1]  [3000/8995]  eta: 0:33:48  lr: 0.002000  loss: 0.2022 (0.3023)  loss_classifier: 0.0589 (0.0832)  loss_box_reg: 0.0962 (0.1089)  loss_mask: 0.0074 (0.0072)  loss_objectness: 0.0241 (0.0417)  loss_rpn_box_reg: 0.0057 (0.0613)  time: 0.3371  data: 0.0375  max mem: 2034\nEpoch: [1]  [4000/8995]  eta: 0:28:07  lr: 0.002000  loss: 0.1466 (0.2975)  loss_classifier: 0.0544 (0.0824)  loss_box_reg: 0.0820 (0.1081)  loss_mask: 0.0077 (0.0072)  loss_objectness: 0.0108 (0.0407)  loss_rpn_box_reg: 0.0040 (0.0590)  time: 0.3473  data: 0.0375  max mem: 2034\nEpoch: [1]  [5000/8995]  eta: 0:22:26  lr: 0.002000  loss: 0.2819 (0.2971)  loss_classifier: 0.1028 (0.0823)  loss_box_reg: 0.1198 (0.1088)  loss_mask: 0.0062 (0.0072)  loss_objectness: 0.0288 (0.0397)  loss_rpn_box_reg: 0.0149 (0.0590)  time: 0.3406  data: 0.0312  max mem: 2034\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}